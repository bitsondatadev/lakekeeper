{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lakekeeper","text":"<p>Lakekeeper is a secure, fast and easy to use Apache Iceberg REST Catalog implementation written in Rust. Start by reading Getting Started, then check Documentation &amp; Concepts for more details information.</p> <p> </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Written in Rust: Single all-in-one binary - no JVM or Python env required.</li> <li>Storage Access Management: Lakekeeper secures access to your data using Vended-Credentials and remote signing for S3. All major Hyperscalers (AWS, Azure, GCP) as well as on-premise deployments with S3 are supported.</li> <li>Openid Provider Integration: Use your own identity provider for authentication, just set <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and you are good to go.</li> <li>Native Kubernetes Integration: Use our helm chart to easily deploy high available setups and natively authenticate kubernetes service accounts with Lakekeeper. Kubernetes and OpenID authentication can be used simultaneously. A Kubernetes Operator is currently in development.</li> <li>Change Events: Built-in support to emit change events (CloudEvents), which enables you to react to any change that happen to your tables.</li> <li>Change Approval: Changes can also be prohibited by external systems. This can be used to prohibit changes to tables that would invalidate Data Contracts, Quality SLOs etc. Simply integrate with your own change approval via our <code>ContractVerification</code> trait.</li> <li>Multi-Tenant capable: A single deployment of Lakekeeper can serve multiple projects - all with a single entrypoint. Each project itself supports multiple Warehouses to which compute engines can connect.</li> <li>Customizable: Lakekeeper is meant to be extended. We expose the Database implementation (<code>Catalog</code>), <code>SecretsStore</code>, <code>Authorizer</code>, Events (<code>CloudEventBackend</code>) and <code>ContractVerification</code> as interfaces (Traits). This allows you to tap into any access management system of your company or stream change events to any system you like - simply by implementing a handful methods.</li> <li>Well-Tested: Integration-tested with <code>spark</code>, <code>pyiceberg</code>, <code>trino</code> and <code>starrocks</code>.</li> <li>High Available &amp; Horizontally Scalable: There is no local state - the catalog can be scaled horizontally easily.</li> <li>Fine Grained Access (FGA): Lakekeeper's default Authorization system leverages OpenFGA. If your company already has a different system in place, you can integrate with it by implementing a handful of methods in the <code>Authorizer</code> trait.</li> </ul> <p>If you are missing something, we would love to hear about it in a Github Issue.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>There are multiple ways to deploy Lakekeeper. You can use one of our self-contained examples, deploy on Kubernetes or deploy the Binary directly.</p>"},{"location":"getting-started/#deployment","title":"Deployment","text":""},{"location":"getting-started/#option-1-examples","title":"Option 1: \ud83d\udc33 Examples","text":"<p>Note</p> <p>Our docker compose examples are not designed to be used with compute outside of the docker network (e.g. external Spark).</p> <p>All docker expose examples come with batteries included (Identity Provider, Storage (S3), Query Engines, Jupyter) but are not accessible (by default) for compute outside of the docker network.</p> \ud83d\udc33 With Authentication &amp; Authorization\ud83d\udc33 Without Authentication <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose/access-control\ndocker-compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose/minimal\ndocker-compose up -d\n</code></pre> <p>Then open your browser and head to <code>localhost:8888</code> to load the example Jupyter notebooks or head to <code>localhost:8080</code> for the Lakekeeper UI.</p>"},{"location":"getting-started/#option-2-kubernetes","title":"Option 2: Kubernetes","text":"<p>We recommend deploying the catalog on Kubernetes using our Helm Chart. Please check the Helm Chart's documentation for possible values. To enable Authentication and Authorization, and external identity provider is required.</p> <p>A community driven Kubernetes Operator is currently in development.</p>"},{"location":"getting-started/#option-3-binary","title":"Option 3: Binary","text":"<p>For single node deployments, you can also download the Binary for your architecture from Github Releases. A basic configuration via environment variables would look something like this:</p> <pre><code>export LAKEKEEPER__BASE_URI=http://localhost:8080\nexport LAKEKEEPER__LISTEN_PORT=8080\nexport LAKEKEEPER__PG_DATABASE_URL_READ=\"postgres://postgres_user:postgres_urlencoded_password@hostname:5432/catalog_database\"\nexport LAKEKEEPER__PG_DATABASE_URL_WRITE=\"postgres://postgres_user:postgres_urlencoded_password@hostname:5432/catalog_database\"\nexport LAKEKEEPER__PG_ENCRYPTION_KEY=\"MySecretEncryptionKeyThatIBetterNotLoose\"\n\n./lakekeeper migrate\n./lakekeeper serve\n</code></pre>"},{"location":"getting-started/#first-steps","title":"First Steps","text":"<p>Now that the catalog is up-and-running, the following endpoints are available:</p> <ol> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;</code> - the UI</li> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;/catalog</code> is the Iceberg REST API</li> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;/management</code> contains the management API</li> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;/swagger-ui</code> hosts Swagger to inspect the API specifications</li> </ol>"},{"location":"getting-started/#bootstrapping","title":"Bootstrapping","text":"<p>Our self-contained docker compose examples are already bootstrapped and require no further actions.</p> <p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. Among others, bootstrapping sets the initial administrator of lakekeeper and creates the first project. Please find more information on bootstrapping in the Bootstrap Docs.</p>"},{"location":"getting-started/#creating-a-warehouse","title":"Creating a Warehouse","text":"<p>Now that the server is running, we need to create a new warehouse. We recommend to do this via the UI. Alternatively, we can use the REST-API directly.</p> <p>For an S3 backed warehouse, create a file called <code>create-warehouse-request.json</code>:</p> <pre><code>{\n  \"warehouse-name\": \"my-warehouse\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"my-example-bucket\",\n    \"key-prefix\": \"optional/path/in/bucket\",\n    \"region\": \"us-east-1\",\n    \"sts-role-arn\": \"arn:aws:iam::....:role/....\",\n    \"sts-enabled\": true,\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>We now create a new Warehouse by POSTing the request to the management API:</p> <pre><code>curl -X POST http://localhost:8080/management/v1/warehouse -H \"Content-Type: application/json\" -d @create-warehouse-request.json\n</code></pre> <p>If you want to use a different storage backend, see the Storage Guide for example configurations.</p>"},{"location":"getting-started/#connect-compute","title":"Connect Compute","text":"<p>That's it - we can now use the catalog:</p> <pre><code>import pandas as pd\nimport pyspark\n\nSPARK_VERSION = pyspark.__version__\nSPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\nICEBERG_VERSION = \"1.6.1\"\n\n# if you use adls as storage backend, you need iceberg-azure instead of iceberg-aws-bundle\nconfiguration = {\n    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": \"demo\",\n    \"spark.sql.catalog.demo\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.demo.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    \"spark.sql.catalog.demo.uri\": \"http://localhost:8080/catalog/\",\n    \"spark.sql.catalog.demo.token\": \"dummy\",\n    \"spark.sql.catalog.demo.warehouse\": \"my-warehouse\",\n}\nspark_conf = pyspark.SparkConf()\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\n\nspark.sql(\"USE demo\")\n\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS my_namespace\")\nprint(f\"\\n\\nCurrently the following namespace exist:\")\nprint(spark.sql(\"SHOW NAMESPACES\").toPandas())\nprint(\"\\n\\n\")\n\nsdf = spark.createDataFrame(\n    pd.DataFrame(\n        [[1, 1.2, \"foo\"], [2, 2.2, \"bar\"]], columns=[\"my_ints\", \"my_floats\", \"strings\"]\n    )\n)\n\nspark.sql(\"DROP TABLE IF EXISTS demo.my_namespace.my_table\")\nspark.sql(\n    \"CREATE TABLE demo.my_namespace.my_table (my_ints INT, my_floats DOUBLE, strings STRING) USING iceberg\"\n)\nsdf.writeTo(\"demo.my_namespace.my_table\").append()\nspark.table(\"demo.my_namespace.my_table\").show()\n</code></pre>"},{"location":"support/","title":"Community","text":"<ul> <li> <p> Connect on Discord</p> <p>Connect with us on Discord to ask all your questions, stay up-to-date with the latest announcements and learn how others are using Lakekeeper.</p> </li> <li> <p> Report Issues &amp; Feature Request</p> <p>Open Feature Requests and report Issues on Github.</p> </li> <li> <p> Enterprise Support</p> <p>Get enterprise support for Lakekeeper in self-hosted or managed environments.</p> </li> </ul>"},{"location":"about/code-of-conduct/","title":"Code of Conduct","text":"<p>Lakekeeper adheres to the Rust Code of Conduct. This describes the minimum behavior expected from all contributors. Instances of violations of the Code of Conduct can be reported by contacting the project team at moderation@hansetag.com.</p> <p>When you contribute code, you affirm that the contribution is your original work and that you license the work to the project under the project's open source license. Whether or not you state this explicitly, by submitting any copyrighted material via pull request, email, or other means you agree to license the material under the project's open source license and warrant that you have the legal authority to do so.</p>"},{"location":"about/license/","title":"License","text":""},{"location":"about/license/#included-projects","title":"Included projects","text":"<p>Themes documentation is based on <code>MkDocs</code>.</p> <ul> <li>MkDocs - View license.</li> </ul> <p>Many thanks to the authors and contributors of <code>MkDocs</code>!</p>"},{"location":"about/license/#apache-license","title":"Apache License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2024 HANSETAG GmbH</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"about/release-notes/","title":"Release Notes","text":"<p>Please find following changes that affect the core lakekeeper library crate. For more information, please also check our Github Releases.</p>"},{"location":"docs/0.4.3/authentication/","title":"Authentication","text":""},{"location":"docs/0.4.3/authorization/","title":"Authorization","text":"<p>Authorization can only be enabled if Authentication is setup as well. Please check the Authentication Docs for more information.</p>"},{"location":"docs/0.4.3/authorization/#grants","title":"Grants","text":"<p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA allows us to implement a powerful permission model with bi-directional inheritance that is required to efficiently manage modern lakehouses with hierarchical namespaces. With our permission model, we try to find the balance between usability and control for administrators.</p> <p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"},{"location":"docs/0.4.3/authorization/#ownership","title":"Ownership","text":"<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"},{"location":"docs/0.4.3/authorization/#server-admin","title":"Server: Admin","text":"<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"},{"location":"docs/0.4.3/authorization/#server-operator","title":"Server: Operator","text":"<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"},{"location":"docs/0.4.3/authorization/#project-security-admin","title":"Project: Security Admin","text":"<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"},{"location":"docs/0.4.3/authorization/#project-data-admin","title":"Project: Data Admin","text":"<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"},{"location":"docs/0.4.3/authorization/#project-admin","title":"Project: Admin","text":"<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"},{"location":"docs/0.4.3/authorization/#project-role-creator","title":"Project: Role Creator","text":"<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"},{"location":"docs/0.4.3/authorization/#describe","title":"Describe","text":"<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"},{"location":"docs/0.4.3/authorization/#select","title":"Select","text":"<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/0.4.3/authorization/#create","title":"Create","text":"<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/0.4.3/authorization/#modify","title":"Modify","text":"<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"},{"location":"docs/0.4.3/authorization/#pass-grants","title":"Pass Grants","text":"<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"},{"location":"docs/0.4.3/authorization/#manage-grants","title":"Manage Grants","text":"<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"},{"location":"docs/0.4.3/authorization/#inheritance","title":"Inheritance","text":"<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"},{"location":"docs/0.4.3/authorization/#managed-access","title":"Managed Access","text":"<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"},{"location":"docs/0.4.3/authorization/#best-practices","title":"Best Practices","text":"<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"},{"location":"docs/0.4.3/bootstrap/","title":"Bootstrap / Initialize","text":"<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. A typical REST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"},{"location":"docs/0.4.3/concepts/","title":"Concepts","text":""},{"location":"docs/0.4.3/concepts/#entity-hierarchy","title":"Entity Hierarchy","text":"<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification.Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>. ToDo: Swagger</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"},{"location":"docs/0.4.3/concepts/#server","title":"Server","text":"<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> may not be changed after the initial bootstrapping or permissions might not work.</p>"},{"location":"docs/0.4.3/concepts/#project","title":"Project","text":"<p>For single-company setups, we recommend using a single Project setup, which is the default. For multi-project/multi-tenant setups, please check our dedicated guide (ToDo: Link). Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping! with the nil UUID.</p>"},{"location":"docs/0.4.3/concepts/#warehouse","title":"Warehouse","text":"<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please find more information on Soft-Deletes here. Please not that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API while child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/0.4.3/concepts/#namespaces","title":"Namespaces","text":"<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/0.4.3/concepts/#tables-views","title":"Tables &amp; Views","text":"<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"},{"location":"docs/0.4.3/concepts/#users","title":"Users","text":"<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code> (Todo check). This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"},{"location":"docs/0.4.3/concepts/#roles","title":"Roles","text":"<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily. Roles can be provisioned automatically using the <code>/management/v1/roles</code> (Todo check) endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"},{"location":"docs/0.4.3/concepts/#soft-deletion","title":"Soft Deletion","text":"<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse.</p>"},{"location":"docs/0.4.3/configuration/","title":"Configuration","text":"<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"},{"location":"docs/0.4.3/configuration/#general","title":"General","text":"Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8080</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8080</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8080</code> Port the Lakekeeper listens on. Default: <code>8080</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>]"},{"location":"docs/0.4.3/configuration/#persistence-store","title":"Persistence Store","text":"<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"},{"location":"docs/0.4.3/configuration/#vault-kv-version-2","title":"Vault KV Version 2","text":"<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"},{"location":"docs/0.4.3/configuration/#task-queues","title":"Task queues","text":"<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"},{"location":"docs/0.4.3/configuration/#nats","title":"Nats","text":"<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"},{"location":"docs/0.4.3/configuration/#authentication","title":"Authentication","text":"<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."},{"location":"docs/0.4.3/configuration/#ssl-dependencies","title":"SSL Dependencies","text":"<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"},{"location":"docs/0.4.3/customize/","title":"Customize","text":"<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"},{"location":"docs/0.4.3/deployment/","title":"Deployment","text":"<p>To get started quickly with the latest version of Lakekeeper check our Getting Started.</p>"},{"location":"docs/0.4.3/deployment/#overview","title":"Overview","text":"<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can Authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul>"},{"location":"docs/0.4.3/deployment/#docker","title":"\ud83d\udc33 Docker","text":"<p>Deploy Lakekeeper using Docker Compose for a quick and easy setup. This method is ideal for local development and testing as well as smaller deployments that don't require high availability. Please check our Examples for simple standalone deployments that come with batteries included (Identity Provider, Storage (S3), Spark, Jupyter) but are not accessible (by default) for compute outside of the docker network. For real-world deployments that are usable for external compute, please continue here.</p> <p>To run Lakekeeper with Authentication and Authorization an external Identity Provider is required. Please check the Authentication Guide for more information.</p> With Authentication &amp; AuthorizationWithout Authentication <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\nexport LAKEKEEPER__OPENID_PROVIDER_URI=\"&lt;open-id-provider-url&gt;\"\ndocker-compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\ndocker-compose up -d\n</code></pre> <p>The services are now starting up and running in the background. To stop all services run: <pre><code>docker compose stop\n</code></pre></p>"},{"location":"docs/0.4.3/deployment/#kubernetes","title":"\u2638\ufe0f Kubernetes","text":"<p>Deploy Lakekeeper on Kubernetes for a scalable and production-ready setup. Use the provided Helm chart for easy deployment.</p> <pre><code>helm repo add lakekeeper https://lakekeeper.github.io/lakekeeper-charts/\nhelm install my-lakekeeper lakekeeper/lakekeeper\n</code></pre> <p>Please check the Helm Chart and its values.yaml for configuration options.</p>"},{"location":"docs/0.4.3/developer-guide/","title":"Developer Guide","text":"<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"},{"location":"docs/0.4.3/developer-guide/#quickstart","title":"Quickstart","text":"<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"},{"location":"docs/0.4.3/developer-guide/#developing-with-docker-compose","title":"Developing with docker compose","text":"<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"},{"location":"docs/0.4.3/developer-guide/#working-with-sqlx","title":"Working with SQLx","text":"<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"},{"location":"docs/0.4.3/developer-guide/#kv2-vault","title":"KV2 / Vault","text":"<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"},{"location":"docs/0.4.3/developer-guide/#test-cloud-storage-profiles","title":"Test cloud storage profiles","text":"<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS s3 &amp; Azure Datalake Storage Gen 2, you need to set the following environment variables for more information take a look at the storage guide (STORAGE.md), a sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"},{"location":"docs/0.4.3/developer-guide/#running-integration-test","title":"Running integration test","text":"<p>Please check the Integration Test Docs.</p>"},{"location":"docs/0.4.3/production/","title":"Production Checklist","text":"<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a '/health' endpoint that should to determine the current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"},{"location":"docs/0.4.3/storage/","title":"Storage Profiles","text":"<p>Currently, we support the following storage profiles:</p> <ul> <li>S3 (tested with aws &amp; minio)</li> <li>Azure Data Lake Storage Gen 2</li> </ul>"},{"location":"docs/0.4.3/storage/#s3","title":"S3","text":"<p>We support remote signing and vended-credentials with minio &amp; aws. Remote signing works for both out of the box, vended-credentials needs some additional setup for aws.</p>"},{"location":"docs/0.4.3/storage/#aws","title":"AWS","text":"<p>To use vended-credentials with aws, your storage profile needs to contain</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": null,\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": \"arn:aws:iam::....:role/....\",\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>The <code>sts_role_arn</code> is the role the temporary credentials are assume.</p> <p>The storage-credential, i.e. <code>aws-access-key-id</code> &amp; <code>aws-secret-access-key</code> should belong to a user which has <code>[s3:GetObject, s3:PutObject, s3:DeleteObject]</code> and the <code>[sts:AssumeRole]</code> permissions.</p> <p>The <code>sts_role_arn</code> also needs a trust relationship to the user, e.g.:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::....:user/....\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/0.4.3/storage/#minio","title":"Minio","text":"<p>For minio, the setup does not require any additional configuration, we use AssumeRole using the provided credentials in the storage profile to get temporary credentials. Any provided <code>sts_role_arn</code> is ignored.</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": \"http://localhost:9000\",\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": null,\n    \"flavor\": \"minio\",\n    \"sts-enabled\": false\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"minio-root-user\",\n    \"aws-secret-access-key\": \"minio-root-password\"\n  }\n}\n</code></pre>"},{"location":"docs/0.4.3/storage/#azure-data-lake-storage-gen-2","title":"Azure Data Lake Storage Gen 2","text":"<p>For Azure Data Lake Storage Gen 2, the app registration your client credentials belong to needs to have the following permissions for the storage account (<code>account-name</code>):</p> <ul> <li><code>Storage Blob Data Contributor</code></li> <li><code>Storage Blob Delegator</code></li> </ul> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"azdls\",\n    \"filesystem\": \"...\",\n    \"key-prefix\": \"...\",\n    \"account-name\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"az\",\n    \"credential-type\": \"client-credentials\",\n    \"client-id\": \"...\",\n    \"client-secret\": \"...\",\n    \"tenant-id\": \"...\"\n  }\n}\n</code></pre>"},{"location":"docs/0.4.3/storage/#gcs","title":"GCS","text":"<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"},{"location":"docs/0.4.3/docs/authentication/","title":"Authentication","text":""},{"location":"docs/0.4.3/docs/authorization/","title":"Authorization","text":"<p>Authorization can only be enabled if Authentication is setup as well. Please check the Authentication Docs for more information.</p>"},{"location":"docs/0.4.3/docs/authorization/#grants","title":"Grants","text":"<p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA allows us to implement a powerful permission model with bi-directional inheritance that is required to efficiently manage modern lakehouses with hierarchical namespaces. With our permission model, we try to find the balance between usability and control for administrators.</p> <p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"},{"location":"docs/0.4.3/docs/authorization/#ownership","title":"Ownership","text":"<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"},{"location":"docs/0.4.3/docs/authorization/#server-admin","title":"Server: Admin","text":"<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"},{"location":"docs/0.4.3/docs/authorization/#server-operator","title":"Server: Operator","text":"<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"},{"location":"docs/0.4.3/docs/authorization/#project-security-admin","title":"Project: Security Admin","text":"<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"},{"location":"docs/0.4.3/docs/authorization/#project-data-admin","title":"Project: Data Admin","text":"<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"},{"location":"docs/0.4.3/docs/authorization/#project-admin","title":"Project: Admin","text":"<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"},{"location":"docs/0.4.3/docs/authorization/#project-role-creator","title":"Project: Role Creator","text":"<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"},{"location":"docs/0.4.3/docs/authorization/#describe","title":"Describe","text":"<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"},{"location":"docs/0.4.3/docs/authorization/#select","title":"Select","text":"<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/0.4.3/docs/authorization/#create","title":"Create","text":"<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/0.4.3/docs/authorization/#modify","title":"Modify","text":"<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"},{"location":"docs/0.4.3/docs/authorization/#pass-grants","title":"Pass Grants","text":"<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"},{"location":"docs/0.4.3/docs/authorization/#manage-grants","title":"Manage Grants","text":"<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"},{"location":"docs/0.4.3/docs/authorization/#inheritance","title":"Inheritance","text":"<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"},{"location":"docs/0.4.3/docs/authorization/#managed-access","title":"Managed Access","text":"<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"},{"location":"docs/0.4.3/docs/authorization/#best-practices","title":"Best Practices","text":"<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"},{"location":"docs/0.4.3/docs/bootstrap/","title":"Bootstrap / Initialize","text":"<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. A typical REST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"},{"location":"docs/0.4.3/docs/concepts/","title":"Concepts","text":""},{"location":"docs/0.4.3/docs/concepts/#entity-hierarchy","title":"Entity Hierarchy","text":"<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification.Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>. ToDo: Swagger</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"},{"location":"docs/0.4.3/docs/concepts/#server","title":"Server","text":"<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> may not be changed after the initial bootstrapping or permissions might not work.</p>"},{"location":"docs/0.4.3/docs/concepts/#project","title":"Project","text":"<p>For single-company setups, we recommend using a single Project setup, which is the default. For multi-project/multi-tenant setups, please check our dedicated guide (ToDo: Link). Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping! with the nil UUID.</p>"},{"location":"docs/0.4.3/docs/concepts/#warehouse","title":"Warehouse","text":"<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please find more information on Soft-Deletes here. Please not that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API while child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/0.4.3/docs/concepts/#namespaces","title":"Namespaces","text":"<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/0.4.3/docs/concepts/#tables-views","title":"Tables &amp; Views","text":"<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"},{"location":"docs/0.4.3/docs/concepts/#users","title":"Users","text":"<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code> (Todo check). This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"},{"location":"docs/0.4.3/docs/concepts/#roles","title":"Roles","text":"<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily. Roles can be provisioned automatically using the <code>/management/v1/roles</code> (Todo check) endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"},{"location":"docs/0.4.3/docs/concepts/#soft-deletion","title":"Soft Deletion","text":"<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse.</p>"},{"location":"docs/0.4.3/docs/configuration/","title":"Configuration","text":"<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"},{"location":"docs/0.4.3/docs/configuration/#general","title":"General","text":"Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8080</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8080</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8080</code> Port the Lakekeeper listens on. Default: <code>8080</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>]"},{"location":"docs/0.4.3/docs/configuration/#persistence-store","title":"Persistence Store","text":"<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"},{"location":"docs/0.4.3/docs/configuration/#vault-kv-version-2","title":"Vault KV Version 2","text":"<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"},{"location":"docs/0.4.3/docs/configuration/#task-queues","title":"Task queues","text":"<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"},{"location":"docs/0.4.3/docs/configuration/#nats","title":"Nats","text":"<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"},{"location":"docs/0.4.3/docs/configuration/#authentication","title":"Authentication","text":"<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."},{"location":"docs/0.4.3/docs/configuration/#ssl-dependencies","title":"SSL Dependencies","text":"<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"},{"location":"docs/0.4.3/docs/customize/","title":"Customize","text":"<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"},{"location":"docs/0.4.3/docs/deployment/","title":"Deployment","text":"<p>To get started quickly with the latest version of Lakekeeper check our Getting Started.</p>"},{"location":"docs/0.4.3/docs/deployment/#overview","title":"Overview","text":"<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can Authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul>"},{"location":"docs/0.4.3/docs/deployment/#docker","title":"\ud83d\udc33 Docker","text":"<p>Deploy Lakekeeper using Docker Compose for a quick and easy setup. This method is ideal for local development and testing as well as smaller deployments that don't require high availability. Please check our Examples for simple standalone deployments that come with batteries included (Identity Provider, Storage (S3), Spark, Jupyter) but are not accessible (by default) for compute outside of the docker network. For real-world deployments that are usable for external compute, please continue here.</p> <p>To run Lakekeeper with Authentication and Authorization an external Identity Provider is required. Please check the Authentication Guide for more information.</p> With Authentication &amp; AuthorizationWithout Authentication <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\nexport LAKEKEEPER__OPENID_PROVIDER_URI=\"&lt;open-id-provider-url&gt;\"\ndocker-compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\ndocker-compose up -d\n</code></pre> <p>The services are now starting up and running in the background. To stop all services run: <pre><code>docker compose stop\n</code></pre></p>"},{"location":"docs/0.4.3/docs/deployment/#kubernetes","title":"\u2638\ufe0f Kubernetes","text":"<p>Deploy Lakekeeper on Kubernetes for a scalable and production-ready setup. Use the provided Helm chart for easy deployment.</p> <pre><code>helm repo add lakekeeper https://lakekeeper.github.io/lakekeeper-charts/\nhelm install my-lakekeeper lakekeeper/lakekeeper\n</code></pre> <p>Please check the Helm Chart and its values.yaml for configuration options.</p>"},{"location":"docs/0.4.3/docs/developer-guide/","title":"Developer Guide","text":"<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"},{"location":"docs/0.4.3/docs/developer-guide/#quickstart","title":"Quickstart","text":"<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"},{"location":"docs/0.4.3/docs/developer-guide/#developing-with-docker-compose","title":"Developing with docker compose","text":"<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"},{"location":"docs/0.4.3/docs/developer-guide/#working-with-sqlx","title":"Working with SQLx","text":"<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"},{"location":"docs/0.4.3/docs/developer-guide/#kv2-vault","title":"KV2 / Vault","text":"<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"},{"location":"docs/0.4.3/docs/developer-guide/#test-cloud-storage-profiles","title":"Test cloud storage profiles","text":"<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS s3 &amp; Azure Datalake Storage Gen 2, you need to set the following environment variables for more information take a look at the storage guide (STORAGE.md), a sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"},{"location":"docs/0.4.3/docs/developer-guide/#running-integration-test","title":"Running integration test","text":"<p>Please check the Integration Test Docs.</p>"},{"location":"docs/0.4.3/docs/production/","title":"Production Checklist","text":"<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a '/health' endpoint that should to determine the current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"},{"location":"docs/0.4.3/docs/storage/","title":"Storage Profiles","text":"<p>Currently, we support the following storage profiles:</p> <ul> <li>S3 (tested with aws &amp; minio)</li> <li>Azure Data Lake Storage Gen 2</li> </ul>"},{"location":"docs/0.4.3/docs/storage/#s3","title":"S3","text":"<p>We support remote signing and vended-credentials with minio &amp; aws. Remote signing works for both out of the box, vended-credentials needs some additional setup for aws.</p>"},{"location":"docs/0.4.3/docs/storage/#aws","title":"AWS","text":"<p>To use vended-credentials with aws, your storage profile needs to contain</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": null,\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": \"arn:aws:iam::....:role/....\",\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>The <code>sts_role_arn</code> is the role the temporary credentials are assume.</p> <p>The storage-credential, i.e. <code>aws-access-key-id</code> &amp; <code>aws-secret-access-key</code> should belong to a user which has <code>[s3:GetObject, s3:PutObject, s3:DeleteObject]</code> and the <code>[sts:AssumeRole]</code> permissions.</p> <p>The <code>sts_role_arn</code> also needs a trust relationship to the user, e.g.:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::....:user/....\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/0.4.3/docs/storage/#minio","title":"Minio","text":"<p>For minio, the setup does not require any additional configuration, we use AssumeRole using the provided credentials in the storage profile to get temporary credentials. Any provided <code>sts_role_arn</code> is ignored.</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": \"http://localhost:9000\",\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": null,\n    \"flavor\": \"minio\",\n    \"sts-enabled\": false\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"minio-root-user\",\n    \"aws-secret-access-key\": \"minio-root-password\"\n  }\n}\n</code></pre>"},{"location":"docs/0.4.3/docs/storage/#azure-data-lake-storage-gen-2","title":"Azure Data Lake Storage Gen 2","text":"<p>For Azure Data Lake Storage Gen 2, the app registration your client credentials belong to needs to have the following permissions for the storage account (<code>account-name</code>):</p> <ul> <li><code>Storage Blob Data Contributor</code></li> <li><code>Storage Blob Delegator</code></li> </ul> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"azdls\",\n    \"filesystem\": \"...\",\n    \"key-prefix\": \"...\",\n    \"account-name\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"az\",\n    \"credential-type\": \"client-credentials\",\n    \"client-id\": \"...\",\n    \"client-secret\": \"...\",\n    \"tenant-id\": \"...\"\n  }\n}\n</code></pre>"},{"location":"docs/0.4.3/docs/storage/#gcs","title":"GCS","text":"<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"},{"location":"docs/latest/authentication/","title":"Authentication","text":""},{"location":"docs/latest/authorization/","title":"Authorization","text":"<p>Authorization can only be enabled if Authentication is setup as well. Please check the Authentication Docs for more information.</p>"},{"location":"docs/latest/authorization/#grants","title":"Grants","text":"<p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA allows us to implement a powerful permission model with bi-directional inheritance that is required to efficiently manage modern lakehouses with hierarchical namespaces. With our permission model, we try to find the balance between usability and control for administrators.</p> <p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"},{"location":"docs/latest/authorization/#ownership","title":"Ownership","text":"<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"},{"location":"docs/latest/authorization/#server-admin","title":"Server: Admin","text":"<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"},{"location":"docs/latest/authorization/#server-operator","title":"Server: Operator","text":"<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"},{"location":"docs/latest/authorization/#project-security-admin","title":"Project: Security Admin","text":"<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"},{"location":"docs/latest/authorization/#project-data-admin","title":"Project: Data Admin","text":"<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"},{"location":"docs/latest/authorization/#project-admin","title":"Project: Admin","text":"<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"},{"location":"docs/latest/authorization/#project-role-creator","title":"Project: Role Creator","text":"<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"},{"location":"docs/latest/authorization/#describe","title":"Describe","text":"<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"},{"location":"docs/latest/authorization/#select","title":"Select","text":"<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/latest/authorization/#create","title":"Create","text":"<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/latest/authorization/#modify","title":"Modify","text":"<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"},{"location":"docs/latest/authorization/#pass-grants","title":"Pass Grants","text":"<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"},{"location":"docs/latest/authorization/#manage-grants","title":"Manage Grants","text":"<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"},{"location":"docs/latest/authorization/#inheritance","title":"Inheritance","text":"<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"},{"location":"docs/latest/authorization/#managed-access","title":"Managed Access","text":"<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"},{"location":"docs/latest/authorization/#best-practices","title":"Best Practices","text":"<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"},{"location":"docs/latest/bootstrap/","title":"Bootstrap / Initialize","text":"<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. A typical REST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"},{"location":"docs/latest/concepts/","title":"Concepts","text":""},{"location":"docs/latest/concepts/#entity-hierarchy","title":"Entity Hierarchy","text":"<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification.Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>. ToDo: Swagger</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"},{"location":"docs/latest/concepts/#server","title":"Server","text":"<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> may not be changed after the initial bootstrapping or permissions might not work.</p>"},{"location":"docs/latest/concepts/#project","title":"Project","text":"<p>For single-company setups, we recommend using a single Project setup, which is the default. For multi-project/multi-tenant setups, please check our dedicated guide (ToDo: Link). Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping! with the nil UUID.</p>"},{"location":"docs/latest/concepts/#warehouse","title":"Warehouse","text":"<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please find more information on Soft-Deletes here. Please not that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API while child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/latest/concepts/#namespaces","title":"Namespaces","text":"<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/latest/concepts/#tables-views","title":"Tables &amp; Views","text":"<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"},{"location":"docs/latest/concepts/#users","title":"Users","text":"<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code> (Todo check). This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"},{"location":"docs/latest/concepts/#roles","title":"Roles","text":"<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily. Roles can be provisioned automatically using the <code>/management/v1/roles</code> (Todo check) endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"},{"location":"docs/latest/concepts/#soft-deletion","title":"Soft Deletion","text":"<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse.</p>"},{"location":"docs/latest/configuration/","title":"Configuration","text":"<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"},{"location":"docs/latest/configuration/#general","title":"General","text":"Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8080</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8080</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8080</code> Port the Lakekeeper listens on. Default: <code>8080</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>]"},{"location":"docs/latest/configuration/#persistence-store","title":"Persistence Store","text":"<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"},{"location":"docs/latest/configuration/#vault-kv-version-2","title":"Vault KV Version 2","text":"<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"},{"location":"docs/latest/configuration/#task-queues","title":"Task queues","text":"<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"},{"location":"docs/latest/configuration/#nats","title":"Nats","text":"<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"},{"location":"docs/latest/configuration/#authentication","title":"Authentication","text":"<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."},{"location":"docs/latest/configuration/#ssl-dependencies","title":"SSL Dependencies","text":"<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"},{"location":"docs/latest/customize/","title":"Customize","text":"<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"},{"location":"docs/latest/deployment/","title":"Deployment","text":"<p>To get started quickly with the latest version of Lakekeeper check our Getting Started.</p>"},{"location":"docs/latest/deployment/#overview","title":"Overview","text":"<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can Authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul>"},{"location":"docs/latest/deployment/#docker","title":"\ud83d\udc33 Docker","text":"<p>Deploy Lakekeeper using Docker Compose for a quick and easy setup. This method is ideal for local development and testing as well as smaller deployments that don't require high availability. Please check our Examples for simple standalone deployments that come with batteries included (Identity Provider, Storage (S3), Spark, Jupyter) but are not accessible (by default) for compute outside of the docker network. For real-world deployments that are usable for external compute, please continue here.</p> <p>To run Lakekeeper with Authentication and Authorization an external Identity Provider is required. Please check the Authentication Guide for more information.</p> With Authentication &amp; AuthorizationWithout Authentication <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\nexport LAKEKEEPER__OPENID_PROVIDER_URI=\"&lt;open-id-provider-url&gt;\"\ndocker-compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\ndocker-compose up -d\n</code></pre> <p>The services are now starting up and running in the background. To stop all services run: <pre><code>docker compose stop\n</code></pre></p>"},{"location":"docs/latest/deployment/#kubernetes","title":"\u2638\ufe0f Kubernetes","text":"<p>Deploy Lakekeeper on Kubernetes for a scalable and production-ready setup. Use the provided Helm chart for easy deployment.</p> <pre><code>helm repo add lakekeeper https://lakekeeper.github.io/lakekeeper-charts/\nhelm install my-lakekeeper lakekeeper/lakekeeper\n</code></pre> <p>Please check the Helm Chart and its values.yaml for configuration options.</p>"},{"location":"docs/latest/developer-guide/","title":"Developer Guide","text":"<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"},{"location":"docs/latest/developer-guide/#quickstart","title":"Quickstart","text":"<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"},{"location":"docs/latest/developer-guide/#developing-with-docker-compose","title":"Developing with docker compose","text":"<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"},{"location":"docs/latest/developer-guide/#working-with-sqlx","title":"Working with SQLx","text":"<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"},{"location":"docs/latest/developer-guide/#kv2-vault","title":"KV2 / Vault","text":"<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"},{"location":"docs/latest/developer-guide/#test-cloud-storage-profiles","title":"Test cloud storage profiles","text":"<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS s3 &amp; Azure Datalake Storage Gen 2, you need to set the following environment variables for more information take a look at the storage guide (STORAGE.md), a sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"},{"location":"docs/latest/developer-guide/#running-integration-test","title":"Running integration test","text":"<p>Please check the Integration Test Docs.</p>"},{"location":"docs/latest/production/","title":"Production Checklist","text":"<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a '/health' endpoint that should to determine the current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"},{"location":"docs/latest/storage/","title":"Storage Profiles","text":"<p>Currently, we support the following storage profiles:</p> <ul> <li>S3 (tested with aws &amp; minio)</li> <li>Azure Data Lake Storage Gen 2</li> </ul>"},{"location":"docs/latest/storage/#s3","title":"S3","text":"<p>We support remote signing and vended-credentials with minio &amp; aws. Remote signing works for both out of the box, vended-credentials needs some additional setup for aws.</p>"},{"location":"docs/latest/storage/#aws","title":"AWS","text":"<p>To use vended-credentials with aws, your storage profile needs to contain</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": null,\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": \"arn:aws:iam::....:role/....\",\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>The <code>sts_role_arn</code> is the role the temporary credentials are assume.</p> <p>The storage-credential, i.e. <code>aws-access-key-id</code> &amp; <code>aws-secret-access-key</code> should belong to a user which has <code>[s3:GetObject, s3:PutObject, s3:DeleteObject]</code> and the <code>[sts:AssumeRole]</code> permissions.</p> <p>The <code>sts_role_arn</code> also needs a trust relationship to the user, e.g.:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::....:user/....\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/latest/storage/#minio","title":"Minio","text":"<p>For minio, the setup does not require any additional configuration, we use AssumeRole using the provided credentials in the storage profile to get temporary credentials. Any provided <code>sts_role_arn</code> is ignored.</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": \"http://localhost:9000\",\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": null,\n    \"flavor\": \"minio\",\n    \"sts-enabled\": false\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"minio-root-user\",\n    \"aws-secret-access-key\": \"minio-root-password\"\n  }\n}\n</code></pre>"},{"location":"docs/latest/storage/#azure-data-lake-storage-gen-2","title":"Azure Data Lake Storage Gen 2","text":"<p>For Azure Data Lake Storage Gen 2, the app registration your client credentials belong to needs to have the following permissions for the storage account (<code>account-name</code>):</p> <ul> <li><code>Storage Blob Data Contributor</code></li> <li><code>Storage Blob Delegator</code></li> </ul> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"azdls\",\n    \"filesystem\": \"...\",\n    \"key-prefix\": \"...\",\n    \"account-name\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"az\",\n    \"credential-type\": \"client-credentials\",\n    \"client-id\": \"...\",\n    \"client-secret\": \"...\",\n    \"tenant-id\": \"...\"\n  }\n}\n</code></pre>"},{"location":"docs/latest/storage/#gcs","title":"GCS","text":"<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"},{"location":"docs/latest/docs/authentication/","title":"Authentication","text":""},{"location":"docs/latest/docs/authorization/","title":"Authorization","text":"<p>Authorization can only be enabled if Authentication is setup as well. Please check the Authentication Docs for more information.</p>"},{"location":"docs/latest/docs/authorization/#grants","title":"Grants","text":"<p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA allows us to implement a powerful permission model with bi-directional inheritance that is required to efficiently manage modern lakehouses with hierarchical namespaces. With our permission model, we try to find the balance between usability and control for administrators.</p> <p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"},{"location":"docs/latest/docs/authorization/#ownership","title":"Ownership","text":"<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"},{"location":"docs/latest/docs/authorization/#server-admin","title":"Server: Admin","text":"<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"},{"location":"docs/latest/docs/authorization/#server-operator","title":"Server: Operator","text":"<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"},{"location":"docs/latest/docs/authorization/#project-security-admin","title":"Project: Security Admin","text":"<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"},{"location":"docs/latest/docs/authorization/#project-data-admin","title":"Project: Data Admin","text":"<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"},{"location":"docs/latest/docs/authorization/#project-admin","title":"Project: Admin","text":"<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"},{"location":"docs/latest/docs/authorization/#project-role-creator","title":"Project: Role Creator","text":"<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"},{"location":"docs/latest/docs/authorization/#describe","title":"Describe","text":"<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"},{"location":"docs/latest/docs/authorization/#select","title":"Select","text":"<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/latest/docs/authorization/#create","title":"Create","text":"<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/latest/docs/authorization/#modify","title":"Modify","text":"<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"},{"location":"docs/latest/docs/authorization/#pass-grants","title":"Pass Grants","text":"<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"},{"location":"docs/latest/docs/authorization/#manage-grants","title":"Manage Grants","text":"<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"},{"location":"docs/latest/docs/authorization/#inheritance","title":"Inheritance","text":"<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"},{"location":"docs/latest/docs/authorization/#managed-access","title":"Managed Access","text":"<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"},{"location":"docs/latest/docs/authorization/#best-practices","title":"Best Practices","text":"<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"},{"location":"docs/latest/docs/bootstrap/","title":"Bootstrap / Initialize","text":"<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. A typical REST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"},{"location":"docs/latest/docs/concepts/","title":"Concepts","text":""},{"location":"docs/latest/docs/concepts/#entity-hierarchy","title":"Entity Hierarchy","text":"<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification.Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>. ToDo: Swagger</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"},{"location":"docs/latest/docs/concepts/#server","title":"Server","text":"<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> may not be changed after the initial bootstrapping or permissions might not work.</p>"},{"location":"docs/latest/docs/concepts/#project","title":"Project","text":"<p>For single-company setups, we recommend using a single Project setup, which is the default. For multi-project/multi-tenant setups, please check our dedicated guide (ToDo: Link). Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping! with the nil UUID.</p>"},{"location":"docs/latest/docs/concepts/#warehouse","title":"Warehouse","text":"<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please find more information on Soft-Deletes here. Please not that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API while child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/latest/docs/concepts/#namespaces","title":"Namespaces","text":"<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/latest/docs/concepts/#tables-views","title":"Tables &amp; Views","text":"<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"},{"location":"docs/latest/docs/concepts/#users","title":"Users","text":"<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code> (Todo check). This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"},{"location":"docs/latest/docs/concepts/#roles","title":"Roles","text":"<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily. Roles can be provisioned automatically using the <code>/management/v1/roles</code> (Todo check) endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"},{"location":"docs/latest/docs/concepts/#soft-deletion","title":"Soft Deletion","text":"<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse.</p>"},{"location":"docs/latest/docs/configuration/","title":"Configuration","text":"<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"},{"location":"docs/latest/docs/configuration/#general","title":"General","text":"Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8080</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8080</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8080</code> Port the Lakekeeper listens on. Default: <code>8080</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>]"},{"location":"docs/latest/docs/configuration/#persistence-store","title":"Persistence Store","text":"<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"},{"location":"docs/latest/docs/configuration/#vault-kv-version-2","title":"Vault KV Version 2","text":"<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"},{"location":"docs/latest/docs/configuration/#task-queues","title":"Task queues","text":"<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"},{"location":"docs/latest/docs/configuration/#nats","title":"Nats","text":"<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"},{"location":"docs/latest/docs/configuration/#authentication","title":"Authentication","text":"<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."},{"location":"docs/latest/docs/configuration/#ssl-dependencies","title":"SSL Dependencies","text":"<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"},{"location":"docs/latest/docs/customize/","title":"Customize","text":"<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"},{"location":"docs/latest/docs/deployment/","title":"Deployment","text":"<p>To get started quickly with the latest version of Lakekeeper check our Getting Started.</p>"},{"location":"docs/latest/docs/deployment/#overview","title":"Overview","text":"<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can Authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul>"},{"location":"docs/latest/docs/deployment/#docker","title":"\ud83d\udc33 Docker","text":"<p>Deploy Lakekeeper using Docker Compose for a quick and easy setup. This method is ideal for local development and testing as well as smaller deployments that don't require high availability. Please check our Examples for simple standalone deployments that come with batteries included (Identity Provider, Storage (S3), Spark, Jupyter) but are not accessible (by default) for compute outside of the docker network. For real-world deployments that are usable for external compute, please continue here.</p> <p>To run Lakekeeper with Authentication and Authorization an external Identity Provider is required. Please check the Authentication Guide for more information.</p> With Authentication &amp; AuthorizationWithout Authentication <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\nexport LAKEKEEPER__OPENID_PROVIDER_URI=\"&lt;open-id-provider-url&gt;\"\ndocker-compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\ndocker-compose up -d\n</code></pre> <p>The services are now starting up and running in the background. To stop all services run: <pre><code>docker compose stop\n</code></pre></p>"},{"location":"docs/latest/docs/deployment/#kubernetes","title":"\u2638\ufe0f Kubernetes","text":"<p>Deploy Lakekeeper on Kubernetes for a scalable and production-ready setup. Use the provided Helm chart for easy deployment.</p> <pre><code>helm repo add lakekeeper https://lakekeeper.github.io/lakekeeper-charts/\nhelm install my-lakekeeper lakekeeper/lakekeeper\n</code></pre> <p>Please check the Helm Chart and its values.yaml for configuration options.</p>"},{"location":"docs/latest/docs/developer-guide/","title":"Developer Guide","text":"<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"},{"location":"docs/latest/docs/developer-guide/#quickstart","title":"Quickstart","text":"<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"},{"location":"docs/latest/docs/developer-guide/#developing-with-docker-compose","title":"Developing with docker compose","text":"<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"},{"location":"docs/latest/docs/developer-guide/#working-with-sqlx","title":"Working with SQLx","text":"<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"},{"location":"docs/latest/docs/developer-guide/#kv2-vault","title":"KV2 / Vault","text":"<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"},{"location":"docs/latest/docs/developer-guide/#test-cloud-storage-profiles","title":"Test cloud storage profiles","text":"<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS s3 &amp; Azure Datalake Storage Gen 2, you need to set the following environment variables for more information take a look at the storage guide (STORAGE.md), a sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"},{"location":"docs/latest/docs/developer-guide/#running-integration-test","title":"Running integration test","text":"<p>Please check the Integration Test Docs.</p>"},{"location":"docs/latest/docs/production/","title":"Production Checklist","text":"<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a '/health' endpoint that should to determine the current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"},{"location":"docs/latest/docs/storage/","title":"Storage Profiles","text":"<p>Currently, we support the following storage profiles:</p> <ul> <li>S3 (tested with aws &amp; minio)</li> <li>Azure Data Lake Storage Gen 2</li> </ul>"},{"location":"docs/latest/docs/storage/#s3","title":"S3","text":"<p>We support remote signing and vended-credentials with minio &amp; aws. Remote signing works for both out of the box, vended-credentials needs some additional setup for aws.</p>"},{"location":"docs/latest/docs/storage/#aws","title":"AWS","text":"<p>To use vended-credentials with aws, your storage profile needs to contain</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": null,\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": \"arn:aws:iam::....:role/....\",\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>The <code>sts_role_arn</code> is the role the temporary credentials are assume.</p> <p>The storage-credential, i.e. <code>aws-access-key-id</code> &amp; <code>aws-secret-access-key</code> should belong to a user which has <code>[s3:GetObject, s3:PutObject, s3:DeleteObject]</code> and the <code>[sts:AssumeRole]</code> permissions.</p> <p>The <code>sts_role_arn</code> also needs a trust relationship to the user, e.g.:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::....:user/....\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/latest/docs/storage/#minio","title":"Minio","text":"<p>For minio, the setup does not require any additional configuration, we use AssumeRole using the provided credentials in the storage profile to get temporary credentials. Any provided <code>sts_role_arn</code> is ignored.</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": \"http://localhost:9000\",\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": null,\n    \"flavor\": \"minio\",\n    \"sts-enabled\": false\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"minio-root-user\",\n    \"aws-secret-access-key\": \"minio-root-password\"\n  }\n}\n</code></pre>"},{"location":"docs/latest/docs/storage/#azure-data-lake-storage-gen-2","title":"Azure Data Lake Storage Gen 2","text":"<p>For Azure Data Lake Storage Gen 2, the app registration your client credentials belong to needs to have the following permissions for the storage account (<code>account-name</code>):</p> <ul> <li><code>Storage Blob Data Contributor</code></li> <li><code>Storage Blob Delegator</code></li> </ul> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"azdls\",\n    \"filesystem\": \"...\",\n    \"key-prefix\": \"...\",\n    \"account-name\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"az\",\n    \"credential-type\": \"client-credentials\",\n    \"client-id\": \"...\",\n    \"client-secret\": \"...\",\n    \"tenant-id\": \"...\"\n  }\n}\n</code></pre>"},{"location":"docs/latest/docs/storage/#gcs","title":"GCS","text":"<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"},{"location":"docs/nightly/authentication/","title":"Authentication","text":""},{"location":"docs/nightly/authorization/","title":"Authorization","text":"<p>Authorization can only be enabled if Authentication is setup as well. Please check the Authentication Docs for more information.</p>"},{"location":"docs/nightly/authorization/#grants","title":"Grants","text":"<p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA allows us to implement a powerful permission model with bi-directional inheritance that is required to efficiently manage modern lakehouses with hierarchical namespaces. With our permission model, we try to find the balance between usability and control for administrators.</p> <p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"},{"location":"docs/nightly/authorization/#ownership","title":"Ownership","text":"<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"},{"location":"docs/nightly/authorization/#server-admin","title":"Server: Admin","text":"<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"},{"location":"docs/nightly/authorization/#server-operator","title":"Server: Operator","text":"<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"},{"location":"docs/nightly/authorization/#project-security-admin","title":"Project: Security Admin","text":"<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"},{"location":"docs/nightly/authorization/#project-data-admin","title":"Project: Data Admin","text":"<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"},{"location":"docs/nightly/authorization/#project-admin","title":"Project: Admin","text":"<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"},{"location":"docs/nightly/authorization/#project-role-creator","title":"Project: Role Creator","text":"<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"},{"location":"docs/nightly/authorization/#describe","title":"Describe","text":"<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"},{"location":"docs/nightly/authorization/#select","title":"Select","text":"<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/nightly/authorization/#create","title":"Create","text":"<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/nightly/authorization/#modify","title":"Modify","text":"<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"},{"location":"docs/nightly/authorization/#pass-grants","title":"Pass Grants","text":"<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"},{"location":"docs/nightly/authorization/#manage-grants","title":"Manage Grants","text":"<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"},{"location":"docs/nightly/authorization/#inheritance","title":"Inheritance","text":"<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"},{"location":"docs/nightly/authorization/#managed-access","title":"Managed Access","text":"<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"},{"location":"docs/nightly/authorization/#best-practices","title":"Best Practices","text":"<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"},{"location":"docs/nightly/bootstrap/","title":"Bootstrap / Initialize","text":"<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. A typical REST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"},{"location":"docs/nightly/concepts/","title":"Concepts","text":""},{"location":"docs/nightly/concepts/#entity-hierarchy","title":"Entity Hierarchy","text":"<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification.Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>. ToDo: Swagger</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"},{"location":"docs/nightly/concepts/#server","title":"Server","text":"<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> may not be changed after the initial bootstrapping or permissions might not work.</p>"},{"location":"docs/nightly/concepts/#project","title":"Project","text":"<p>For single-company setups, we recommend using a single Project setup, which is the default. For multi-project/multi-tenant setups, please check our dedicated guide (ToDo: Link). Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping! with the nil UUID.</p>"},{"location":"docs/nightly/concepts/#warehouse","title":"Warehouse","text":"<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please find more information on Soft-Deletes here. Please not that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API while child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/nightly/concepts/#namespaces","title":"Namespaces","text":"<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/nightly/concepts/#tables-views","title":"Tables &amp; Views","text":"<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"},{"location":"docs/nightly/concepts/#users","title":"Users","text":"<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code> (Todo check). This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"},{"location":"docs/nightly/concepts/#roles","title":"Roles","text":"<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily. Roles can be provisioned automatically using the <code>/management/v1/roles</code> (Todo check) endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"},{"location":"docs/nightly/concepts/#soft-deletion","title":"Soft Deletion","text":"<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse.</p>"},{"location":"docs/nightly/configuration/","title":"Configuration","text":"<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"},{"location":"docs/nightly/configuration/#general","title":"General","text":"Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8080</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8080</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8080</code> Port the Lakekeeper listens on. Default: <code>8080</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>]"},{"location":"docs/nightly/configuration/#persistence-store","title":"Persistence Store","text":"<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"},{"location":"docs/nightly/configuration/#vault-kv-version-2","title":"Vault KV Version 2","text":"<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"},{"location":"docs/nightly/configuration/#task-queues","title":"Task queues","text":"<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"},{"location":"docs/nightly/configuration/#nats","title":"Nats","text":"<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"},{"location":"docs/nightly/configuration/#authentication","title":"Authentication","text":"<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."},{"location":"docs/nightly/configuration/#ssl-dependencies","title":"SSL Dependencies","text":"<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"},{"location":"docs/nightly/customize/","title":"Customize","text":"<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"},{"location":"docs/nightly/deployment/","title":"Deployment","text":"<p>To get started quickly with the latest version of Lakekeeper check our Getting Started.</p>"},{"location":"docs/nightly/deployment/#overview","title":"Overview","text":"<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can Authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul>"},{"location":"docs/nightly/deployment/#docker","title":"\ud83d\udc33 Docker","text":"<p>Deploy Lakekeeper using Docker Compose for a quick and easy setup. This method is ideal for local development and testing as well as smaller deployments that don't require high availability. Please check our Examples for simple standalone deployments that come with batteries included (Identity Provider, Storage (S3), Spark, Jupyter) but are not accessible (by default) for compute outside of the docker network. For real-world deployments that are usable for external compute, please continue here.</p> <p>To run Lakekeeper with Authentication and Authorization an external Identity Provider is required. Please check the Authentication Guide for more information.</p> With Authentication &amp; AuthorizationWithout Authentication <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\nexport LAKEKEEPER__OPENID_PROVIDER_URI=\"&lt;open-id-provider-url&gt;\"\ndocker-compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\ndocker-compose up -d\n</code></pre> <p>The services are now starting up and running in the background. To stop all services run: <pre><code>docker compose stop\n</code></pre></p>"},{"location":"docs/nightly/deployment/#kubernetes","title":"\u2638\ufe0f Kubernetes","text":"<p>Deploy Lakekeeper on Kubernetes for a scalable and production-ready setup. Use the provided Helm chart for easy deployment.</p> <pre><code>helm repo add lakekeeper https://lakekeeper.github.io/lakekeeper-charts/\nhelm install my-lakekeeper lakekeeper/lakekeeper\n</code></pre> <p>Please check the Helm Chart and its values.yaml for configuration options.</p>"},{"location":"docs/nightly/developer-guide/","title":"Developer Guide","text":"<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"},{"location":"docs/nightly/developer-guide/#quickstart","title":"Quickstart","text":"<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"},{"location":"docs/nightly/developer-guide/#developing-with-docker-compose","title":"Developing with docker compose","text":"<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"},{"location":"docs/nightly/developer-guide/#working-with-sqlx","title":"Working with SQLx","text":"<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"},{"location":"docs/nightly/developer-guide/#kv2-vault","title":"KV2 / Vault","text":"<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"},{"location":"docs/nightly/developer-guide/#test-cloud-storage-profiles","title":"Test cloud storage profiles","text":"<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS s3 &amp; Azure Datalake Storage Gen 2, you need to set the following environment variables for more information take a look at the storage guide (STORAGE.md), a sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"},{"location":"docs/nightly/developer-guide/#running-integration-test","title":"Running integration test","text":"<p>Please check the Integration Test Docs.</p>"},{"location":"docs/nightly/production/","title":"Production Checklist","text":"<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a '/health' endpoint that should to determine the current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"},{"location":"docs/nightly/storage/","title":"Storage Profiles","text":"<p>Currently, we support the following storage profiles:</p> <ul> <li>S3 (tested with aws &amp; minio)</li> <li>Azure Data Lake Storage Gen 2</li> </ul>"},{"location":"docs/nightly/storage/#s3","title":"S3","text":"<p>We support remote signing and vended-credentials with minio &amp; aws. Remote signing works for both out of the box, vended-credentials needs some additional setup for aws.</p>"},{"location":"docs/nightly/storage/#aws","title":"AWS","text":"<p>To use vended-credentials with aws, your storage profile needs to contain</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": null,\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": \"arn:aws:iam::....:role/....\",\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>The <code>sts_role_arn</code> is the role the temporary credentials are assume.</p> <p>The storage-credential, i.e. <code>aws-access-key-id</code> &amp; <code>aws-secret-access-key</code> should belong to a user which has <code>[s3:GetObject, s3:PutObject, s3:DeleteObject]</code> and the <code>[sts:AssumeRole]</code> permissions.</p> <p>The <code>sts_role_arn</code> also needs a trust relationship to the user, e.g.:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::....:user/....\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/nightly/storage/#minio","title":"Minio","text":"<p>For minio, the setup does not require any additional configuration, we use AssumeRole using the provided credentials in the storage profile to get temporary credentials. Any provided <code>sts_role_arn</code> is ignored.</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": \"http://localhost:9000\",\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": null,\n    \"flavor\": \"minio\",\n    \"sts-enabled\": false\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"minio-root-user\",\n    \"aws-secret-access-key\": \"minio-root-password\"\n  }\n}\n</code></pre>"},{"location":"docs/nightly/storage/#azure-data-lake-storage-gen-2","title":"Azure Data Lake Storage Gen 2","text":"<p>For Azure Data Lake Storage Gen 2, the app registration your client credentials belong to needs to have the following permissions for the storage account (<code>account-name</code>):</p> <ul> <li><code>Storage Blob Data Contributor</code></li> <li><code>Storage Blob Delegator</code></li> </ul> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"azdls\",\n    \"filesystem\": \"...\",\n    \"key-prefix\": \"...\",\n    \"account-name\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"az\",\n    \"credential-type\": \"client-credentials\",\n    \"client-id\": \"...\",\n    \"client-secret\": \"...\",\n    \"tenant-id\": \"...\"\n  }\n}\n</code></pre>"},{"location":"docs/nightly/storage/#gcs","title":"GCS","text":"<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"},{"location":"docs/nightly/docs/authentication/","title":"Authentication","text":""},{"location":"docs/nightly/docs/authorization/","title":"Authorization","text":"<p>Authorization can only be enabled if Authentication is setup as well. Please check the Authentication Docs for more information.</p>"},{"location":"docs/nightly/docs/authorization/#grants","title":"Grants","text":"<p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA allows us to implement a powerful permission model with bi-directional inheritance that is required to efficiently manage modern lakehouses with hierarchical namespaces. With our permission model, we try to find the balance between usability and control for administrators.</p> <p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"},{"location":"docs/nightly/docs/authorization/#ownership","title":"Ownership","text":"<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"},{"location":"docs/nightly/docs/authorization/#server-admin","title":"Server: Admin","text":"<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"},{"location":"docs/nightly/docs/authorization/#server-operator","title":"Server: Operator","text":"<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"},{"location":"docs/nightly/docs/authorization/#project-security-admin","title":"Project: Security Admin","text":"<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"},{"location":"docs/nightly/docs/authorization/#project-data-admin","title":"Project: Data Admin","text":"<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"},{"location":"docs/nightly/docs/authorization/#project-admin","title":"Project: Admin","text":"<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"},{"location":"docs/nightly/docs/authorization/#project-role-creator","title":"Project: Role Creator","text":"<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"},{"location":"docs/nightly/docs/authorization/#describe","title":"Describe","text":"<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"},{"location":"docs/nightly/docs/authorization/#select","title":"Select","text":"<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/nightly/docs/authorization/#create","title":"Create","text":"<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"},{"location":"docs/nightly/docs/authorization/#modify","title":"Modify","text":"<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"},{"location":"docs/nightly/docs/authorization/#pass-grants","title":"Pass Grants","text":"<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"},{"location":"docs/nightly/docs/authorization/#manage-grants","title":"Manage Grants","text":"<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"},{"location":"docs/nightly/docs/authorization/#inheritance","title":"Inheritance","text":"<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"},{"location":"docs/nightly/docs/authorization/#managed-access","title":"Managed Access","text":"<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"},{"location":"docs/nightly/docs/authorization/#best-practices","title":"Best Practices","text":"<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"},{"location":"docs/nightly/docs/bootstrap/","title":"Bootstrap / Initialize","text":"<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. A typical REST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"},{"location":"docs/nightly/docs/concepts/","title":"Concepts","text":""},{"location":"docs/nightly/docs/concepts/#entity-hierarchy","title":"Entity Hierarchy","text":"<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification.Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>. ToDo: Swagger</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"},{"location":"docs/nightly/docs/concepts/#server","title":"Server","text":"<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> may not be changed after the initial bootstrapping or permissions might not work.</p>"},{"location":"docs/nightly/docs/concepts/#project","title":"Project","text":"<p>For single-company setups, we recommend using a single Project setup, which is the default. For multi-project/multi-tenant setups, please check our dedicated guide (ToDo: Link). Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping! with the nil UUID.</p>"},{"location":"docs/nightly/docs/concepts/#warehouse","title":"Warehouse","text":"<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please find more information on Soft-Deletes here. Please not that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API while child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/nightly/docs/concepts/#namespaces","title":"Namespaces","text":"<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"},{"location":"docs/nightly/docs/concepts/#tables-views","title":"Tables &amp; Views","text":"<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"},{"location":"docs/nightly/docs/concepts/#users","title":"Users","text":"<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code> (Todo check). This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"},{"location":"docs/nightly/docs/concepts/#roles","title":"Roles","text":"<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily. Roles can be provisioned automatically using the <code>/management/v1/roles</code> (Todo check) endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"},{"location":"docs/nightly/docs/concepts/#soft-deletion","title":"Soft Deletion","text":"<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse.</p>"},{"location":"docs/nightly/docs/configuration/","title":"Configuration","text":"<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"},{"location":"docs/nightly/docs/configuration/#general","title":"General","text":"Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8080</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8080</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8080</code> Port the Lakekeeper listens on. Default: <code>8080</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>]"},{"location":"docs/nightly/docs/configuration/#persistence-store","title":"Persistence Store","text":"<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"},{"location":"docs/nightly/docs/configuration/#vault-kv-version-2","title":"Vault KV Version 2","text":"<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"},{"location":"docs/nightly/docs/configuration/#task-queues","title":"Task queues","text":"<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"},{"location":"docs/nightly/docs/configuration/#nats","title":"Nats","text":"<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"},{"location":"docs/nightly/docs/configuration/#authentication","title":"Authentication","text":"<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."},{"location":"docs/nightly/docs/configuration/#ssl-dependencies","title":"SSL Dependencies","text":"<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"},{"location":"docs/nightly/docs/customize/","title":"Customize","text":"<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"},{"location":"docs/nightly/docs/deployment/","title":"Deployment","text":"<p>To get started quickly with the latest version of Lakekeeper check our Getting Started.</p>"},{"location":"docs/nightly/docs/deployment/#overview","title":"Overview","text":"<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can Authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul>"},{"location":"docs/nightly/docs/deployment/#docker","title":"\ud83d\udc33 Docker","text":"<p>Deploy Lakekeeper using Docker Compose for a quick and easy setup. This method is ideal for local development and testing as well as smaller deployments that don't require high availability. Please check our Examples for simple standalone deployments that come with batteries included (Identity Provider, Storage (S3), Spark, Jupyter) but are not accessible (by default) for compute outside of the docker network. For real-world deployments that are usable for external compute, please continue here.</p> <p>To run Lakekeeper with Authentication and Authorization an external Identity Provider is required. Please check the Authentication Guide for more information.</p> With Authentication &amp; AuthorizationWithout Authentication <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\nexport LAKEKEEPER__OPENID_PROVIDER_URI=\"&lt;open-id-provider-url&gt;\"\ndocker-compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd docker-compose\ndocker-compose up -d\n</code></pre> <p>The services are now starting up and running in the background. To stop all services run: <pre><code>docker compose stop\n</code></pre></p>"},{"location":"docs/nightly/docs/deployment/#kubernetes","title":"\u2638\ufe0f Kubernetes","text":"<p>Deploy Lakekeeper on Kubernetes for a scalable and production-ready setup. Use the provided Helm chart for easy deployment.</p> <pre><code>helm repo add lakekeeper https://lakekeeper.github.io/lakekeeper-charts/\nhelm install my-lakekeeper lakekeeper/lakekeeper\n</code></pre> <p>Please check the Helm Chart and its values.yaml for configuration options.</p>"},{"location":"docs/nightly/docs/developer-guide/","title":"Developer Guide","text":"<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"},{"location":"docs/nightly/docs/developer-guide/#quickstart","title":"Quickstart","text":"<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"},{"location":"docs/nightly/docs/developer-guide/#developing-with-docker-compose","title":"Developing with docker compose","text":"<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"},{"location":"docs/nightly/docs/developer-guide/#working-with-sqlx","title":"Working with SQLx","text":"<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"},{"location":"docs/nightly/docs/developer-guide/#kv2-vault","title":"KV2 / Vault","text":"<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"},{"location":"docs/nightly/docs/developer-guide/#test-cloud-storage-profiles","title":"Test cloud storage profiles","text":"<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS s3 &amp; Azure Datalake Storage Gen 2, you need to set the following environment variables for more information take a look at the storage guide (STORAGE.md), a sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"},{"location":"docs/nightly/docs/developer-guide/#running-integration-test","title":"Running integration test","text":"<p>Please check the Integration Test Docs.</p>"},{"location":"docs/nightly/docs/production/","title":"Production Checklist","text":"<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a '/health' endpoint that should to determine the current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"},{"location":"docs/nightly/docs/storage/","title":"Storage Profiles","text":"<p>Currently, we support the following storage profiles:</p> <ul> <li>S3 (tested with aws &amp; minio)</li> <li>Azure Data Lake Storage Gen 2</li> </ul>"},{"location":"docs/nightly/docs/storage/#s3","title":"S3","text":"<p>We support remote signing and vended-credentials with minio &amp; aws. Remote signing works for both out of the box, vended-credentials needs some additional setup for aws.</p>"},{"location":"docs/nightly/docs/storage/#aws","title":"AWS","text":"<p>To use vended-credentials with aws, your storage profile needs to contain</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": null,\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": \"arn:aws:iam::....:role/....\",\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>The <code>sts_role_arn</code> is the role the temporary credentials are assume.</p> <p>The storage-credential, i.e. <code>aws-access-key-id</code> &amp; <code>aws-secret-access-key</code> should belong to a user which has <code>[s3:GetObject, s3:PutObject, s3:DeleteObject]</code> and the <code>[sts:AssumeRole]</code> permissions.</p> <p>The <code>sts_role_arn</code> also needs a trust relationship to the user, e.g.:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::....:user/....\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/nightly/docs/storage/#minio","title":"Minio","text":"<p>For minio, the setup does not require any additional configuration, we use AssumeRole using the provided credentials in the storage profile to get temporary credentials. Any provided <code>sts_role_arn</code> is ignored.</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"examples\",\n    \"key-prefix\": \"initial-warehouse\",\n    \"assume-role-arn\": null,\n    \"endpoint\": \"http://localhost:9000\",\n    \"region\": \"local-01\",\n    \"path-style-access\": true,\n    \"sts_role_arn\": null,\n    \"flavor\": \"minio\",\n    \"sts-enabled\": false\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"minio-root-user\",\n    \"aws-secret-access-key\": \"minio-root-password\"\n  }\n}\n</code></pre>"},{"location":"docs/nightly/docs/storage/#azure-data-lake-storage-gen-2","title":"Azure Data Lake Storage Gen 2","text":"<p>For Azure Data Lake Storage Gen 2, the app registration your client credentials belong to needs to have the following permissions for the storage account (<code>account-name</code>):</p> <ul> <li><code>Storage Blob Data Contributor</code></li> <li><code>Storage Blob Delegator</code></li> </ul> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"azdls\",\n    \"filesystem\": \"...\",\n    \"key-prefix\": \"...\",\n    \"account-name\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"az\",\n    \"credential-type\": \"client-credentials\",\n    \"client-id\": \"...\",\n    \"client-secret\": \"...\",\n    \"tenant-id\": \"...\"\n  }\n}\n</code></pre>"},{"location":"docs/nightly/docs/storage/#gcs","title":"GCS","text":"<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this:</p> <pre><code>{\n  \"warehouse-name\": \"test\",\n  \"project-id\": \"00000000-0000-0000-0000-000000000000\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"}]}